---
title: "Lab 06: Taming data"
description: |
  You can use this template for Lab 06. However, you will need to add the prose descriptions, add code comments, edit the data dictionary, and complete the self-assessment.
author: "Sicheng Wang"
bibliography: ["../bibliography.bib", "../packages.bib"]
---

```{r}
#| label: setup
#| message: false

library(dplyr)        # Inspecting and sub-setting data
library(knitr)        # Creating a table
library(readtext)     # Reading text contents
library(readr)        # Reading and writing data
library(qtalrkit)     # Creating data dictionaries
library(fs)           # Allowing assigning or getting a path to a directory
```

```{r}
#| label: acquire-data
#| message: false
#| echo: false
#| eval: false

# Assigning the link with data to file_url
file_url <- "https://github.com/francojc/activ-es/raw/master/activ-es-v.02/corpus/plain.zip"

# Assigning the directory path to dir_location
dir_location <- "../data/original/actives"

# Downloading the data from the link in file_url, decompressing the data to store it in the path mentioned in dir_location and the user need to confirm that they have permission to use the data
get_compressed_data(
  url = file_url,
  target_dir = dir_location,
  confirmed = TRUE
  )
```

## Data

<!--

- Overview of the data source and the purpose of this script

-->

From the corpus README, the chosen corpus contain the film dialogue from Argentine, Mexican and Spanish productions. The corpus contains the plain text, annotated text, running text with EAGLES tagset and wordlist file which seems can be used to compare Spanish from Argentina, Mexico, and Spain. Knowing how data stored or structured helps us to find the specific data we want or store the data we later derived in the correct directory.

```{r}
#| label: dir-tree
#| echo: false

# Create a tree of the decompressed data structure
dir_tree(path = "../data", recurse = 2) # only show the first two levels
```

### Description

<!--

- the name and/ or source of the data
- the nature of the data
- the acquisition strategy that was used
- the format of the data

-->

```{r}
#| label: tbl-data-origin-file
#| tbl-cap: "Data origin: Actives corpus"
#| tbl-colwidths: [30, 70]
#| message: false

# Read the actives_do.csv file into a table format
read_csv("../data/original/actives_do.csv") |>
  kable() # Create a table
```

*    This corpus is named as `ACTIV-ES: a comparable Spanish corpus comprised of film dialogue from Argentine, Mexican and Spanish productions (v.02)` from [ACTIV-ES](https://github.com/francojc/activ-es) which viewed as category data.

*    By programmatic download the data through R, found that the data contains running text files (.run), Part of Speech tagged files (.pos), and EAGLES tagged files (.eagles).

### Structure

<!--

- the relevant directories and data files
- the metadata and/ or variables to be organized
- the relationships between the data elements
- the idealized format for the curated dataset in a tabular format

-->

*       In `activ-es-v.02/corpus/`, it contains plain text, annotated files and EAGLES tagged files. The title shows the language code, country, year, title, type, genre and IMDb ID. Other variables like word, observed relative frequency and observed relative dispersion. The language code shows the spanish is from Argentina, Mexico, or Spain. The country shows where the texts in publishes come from. The year is when the publishes show up. The title is the name of publishes. The type is what kind of publishes they are. The genre shows the category of the publishes. IMDb ID is the number of the publishes' title. Word is a specific word, observed relative frequency is word occurrence per 100,000 and observed relative dispersion is occurrence of a word per 10 documents.
*    The aimed tabular format would contains 7 coloumns: language code, year, title, type, genre, and IMDb ID.

## Curate

<!-- Overview of the data curation process -->

The data curation process involves three main steps: orientation, data tidying, and, in the case of semi-structured or structured data, preparation for computer reading. Before curation, it is essential to identify and obtain the corpus or dataset of interest, which may involve downloading and decompressing data. During the orientation stage, it is important to thoroughly review the data's origin information and assess its structure. When tidying the data, it is essential to have an idealized structure in mind while aiming to maintain the original data structure as much as possible. This process aids easy data access and interpretation. The specific steps involved in tidying the data may vary depending on the research purposes. However, the primary objective is to enhance the data's informativeness by removing redundant variables and potentially renaming variables to improve clarity.

### Read

<!-- Description ... -->

Given that the files' titles contain information about the variables, it is more efficient to read the filenames in this case.

```{r}
#| label: read-data-actives
#| message: false

# Create a vector doc_vars which contains variables `lang`, `country`, `year`, `title`, `type`, `genre` and `imdbid`
doc_vars <-
  c("lang", "country", "year", "title", "type", "genre", "imdbid")

# Assign the resulting data to actives_tbl
actives_tbl <-
  # read text in files
  readtext(
    file = "../data/original/actives/*.run", # read every file under `../data/original/actives/` whom match the `.run` pattern
    docvarsfrom = "filenames", # extract document variables from filenames
    docvarnames = doc_vars, # the names of the document variables is set in doc_vars
    verbosity = 0 # not show the messages
  ) |>
  as_tibble() # read the data into a tibble format

# preview
actives_tbl
```

### Tidy

<!-- Description ... -->

This step aims to ensure data clarity for future analysis. Despite the presence of the `imdbid` variable, the dataset lacks unique identifiers for individual rows. Therefore, we introduce the `doc_id` variable to provide each row with a distinct identity within the dataset.

```{r}
#| label: tidy-data-actives

# add column information 
actives_tbl <-
  actives_tbl |>
  mutate(
    doc_id = row_number() # add a column named `doc_id` containing the row numbers for each document
  )

# Preview
actives_tbl
```

### Write

<!-- Description ... -->

The curated data can be saved in the `.csv` format, which is commonly used. It is essential to preserve the original data. Therefore, the curated data should be stored separately in the directory like `../data/derived/`, ensuring that the original data remains unaltered.

```{r}
#| label: write-data-actives

# create a csv file
write_csv(
  x = actives_tbl, # make the actives_tbl to be a csv file
  file = "../data/derived/actives_curated.csv" # create the csv file under path
)
```

## Documentation

<!-- Overview of the purpose and structure of the documentation -->

*    In order to enhance the reproducibility of the project, it is crucial to document the data curation process. This involves adding comments within code blocks and providing a description of the process. Not only does this aid researchers in recalling the process and identifying mistakes, but it also enables other individuals who may utilize or reference the research to understand, verify, or gain insights from the project.

*    The documentation process includes writing descriptive prose, adding code comments, and creating a data dictionary detailing dataset variables and their values.

### Data dictionary

<!-- Description ... -->

The data dictionary provides details about the data's variables and their values. It can be generated manually, programmatically, or even with the assistance of AI. In this instance, we opt to programmatically create the data dictionary.

```{r}
#| label: create-data-dictionary
#| message: false

# create a data dictionary
create_data_dictionary(
  data = actives_tbl, # to contain information in actives_tbl
  file_path = "../data/derived/actives_curated_dd.csv" # create the data dictionary under this path
)
```

<!--

Note:

You will need to open and edit the `actives_curated_dd.csv` file to add the descriptions for each variable.

-->

```{r}
#| label: tbl-data-dictionary-show
#| tbl-cap: "Data dictionary: Actives corpus"
#| message: false

# read the csv file actives_curated_dd.csv
read_csv("../data/derived/actives_curated_dd.csv") |>
  kable() # read the csv file into the table format
```

### Project structure

<!-- Description ... -->

The derived data is stored in `../data/derived`, while the curation process is documented in `../process`.

```{r}
#| label: show-directory-structure
#| echo: false

# Create a tree of the project structure
dir_tree("../", recurse = 2) # only show the first two levels
```

## Self-assessment

<!-- Complete the self-assessment -->

Please see `Issues` on Github under this repository.

<!-- Review the .gitignore file to make sure that the data and datasets are not tracked by Git -->

<!-- Commit and push your repo -->
